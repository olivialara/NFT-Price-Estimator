{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17766f7-a7fe-48cf-aa99-e42972b6a926",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Capstone: NFTs\n",
    "## Part VI: Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53944a13-c7ff-431c-b787-b64a69b3b2c1",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0ce68-4ef8-447d-b23b-5380f325338f",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- https://moscow25.medium.com/predicting-cryptopunk-prices-the-case-for-jpegs-e4fc0f0fafd1\n",
    "- https://medium.com/geekculture/cosine-similarity-and-cosine-distance-48eed889a5c4\n",
    "- https://raritytools.medium.com/ranking-rarity-understanding-rarity-calculation-methods-86ceaeb9b98c\n",
    "- https://goodboychan.github.io/python/datacamp/natural_language_processing/2020/07/17/04-TF-IDF-and-similarity-scores.html\n",
    "- citation: https://stackoverflow.com/questions/1894269/how-to-convert-string-representation-of-list-to-a-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e74b1-4e08-47d2-a543-bce7f1b54c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fa447c1-d476-48af-b50f-83f739976f4a",
   "metadata": {},
   "source": [
    "### 3. Get CryptoPunk Owners' Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3aba38-8bd2-4827-89b0-4db0a5215dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "I will web scrap from Larva Labs to get each CryptoPunk's Owner (account #):|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2f0bb-7981-4c82-b554-fd84e6423b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# citation: https://www.larvalabs.com/cryptopunks/leaderboard\n",
    "\n",
    "url = 'https://www.larvalabs.com/cryptopunks/leaderboard#'\n",
    "response = requests.get(url)\n",
    "\n",
    "response.text\n",
    "\n",
    "soup = BeautifulSoup(response.content)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38a3f9-eccc-4835-9cbc-ea940ff5dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result set where class is equal to pinklink\n",
    "small_a = soup.find_all('a', class_=\"pinklink\")\n",
    "\n",
    "# list comprehension to get links of all small_a\n",
    "links = [link.attrs['href'] for link in small_a if link in small_a]\n",
    "\n",
    "# empty accounts list\n",
    "accounts = []\n",
    "\n",
    "# get account number from link\n",
    "for link in links:\n",
    "    accounts.append(link[33:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d182c-83de-43f0-a300-7e043656982f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad4ed26d-9941-4ba4-84cd-e8cd5a65b83e",
   "metadata": {},
   "source": [
    "### 4. Get Each NFT Sale & Picture Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18d87c-727c-4ced-8462-5ff427aa6d65",
   "metadata": {},
   "source": [
    "I will use Open Sea's API to grab each CryptoPunk's Sale and Picture URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d56c7-9458-442b-9ffd-080833a40a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are approx. 3200 crypto punk owners at the time this data was collected\n",
    "print(len(accounts))\n",
    "\n",
    "# I will create 10 sections of the accounts list\n",
    "print(3194/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200497c-b59f-43d0-a2d2-3f5efbd9d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# citation: https://docs.opensea.io/reference/getting-assets\n",
    "\n",
    "all_accounts_info = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for account in accounts:\n",
    "    counter += 1\n",
    "    \n",
    "    url = \"https://api.opensea.io/api/v1/assets?owner=\" + account + \"&order_direction=desc&offset=0&limit=20&collection=cryptopunks\"\n",
    "    \n",
    "    # requests\n",
    "    res = requests.get(url)\n",
    "\n",
    "    # convert the request into a list of dict objects\n",
    "    data = res.json()\n",
    "    \n",
    "    # convert to dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # concat\n",
    "    all_accounts_info = pd.concat([all_accounts_info, df])\n",
    "        \n",
    "    # reset index to correctly order indices\n",
    "    all_accounts_info = all_accounts_info.reset_index().drop(columns = ['index'])\n",
    "    \n",
    "    # add wait time b/c can only call 20 at a time - seconds in parentheses \n",
    "    if counter % 20 == 0:\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2edb0-0605-4c2f-9744-75029c7393ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accounts_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247dedf-e55a-435d-9ae2-144a57beb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dataframe \n",
    "punk_info_df = pd.DataFrame()\n",
    "\n",
    "# create dictionary for each nft\n",
    "for i in range(all_accounts_info.shape[0]):\n",
    "    nft_dict = {key:value for key, value in all_accounts_info.iloc[i][0].items() if ('token_id' == key) | ('num_sales' == key) | ('image_original_url' == key) | ('last_sale' == key)}\n",
    "    \n",
    "    # if empty sale set value to None, otherwise get price\n",
    "    if nft_dict['last_sale'] is None: \n",
    "        total_price = None\n",
    "        usd_price = None\n",
    "        event_timestamp = None\n",
    "    else: \n",
    "        # gets rid of trailing 0's\n",
    "        total_price = str(nft_dict['last_sale']['total_price']).strip(\"0\")\n",
    "        # rounds price\n",
    "        usd_price = np.round(float(nft_dict['last_sale']['payment_token']['usd_price']), 4)\n",
    "        event_timestamp = nft_dict['last_sale']['event_timestamp']\n",
    "        \n",
    "    # print(nft_dict)\n",
    "    \n",
    "    # remake nft dictionary with only certain values\n",
    "    nft_dict = {'token_id': nft_dict['token_id'],\n",
    "                'num_sales': nft_dict['num_sales'],\n",
    "                'image_original_url': nft_dict['image_original_url'],\n",
    "                'event_timestamp': event_timestamp,\n",
    "                'total_price': total_price,\n",
    "                'usd_price': usd_price\n",
    "               }\n",
    "    \n",
    "    # empty nft list\n",
    "    nft_list = []\n",
    "    \n",
    "    # convert nft dic to list\n",
    "    for i in nft_dict.values():\n",
    "        nft_list.append(i)\n",
    "        \n",
    "    # print(nft_list)\n",
    "    \n",
    "    # add nft info to dataframe\n",
    "    punk_info_df = punk_info_df.append(nft_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4883547-9f85-434b-8847-b0668384fc8b",
   "metadata": {},
   "source": [
    "### 5. Create Plot with Range Slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe2a35-ffd5-439a-80b0-0e9c37f81ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time series with range slider\n",
    "\n",
    "punk_1_sold = punk_1_trans[punk_1_trans['trans'] == 'Sold']\n",
    "punk_1_bid = punk_1_trans[punk_1_trans['trans'] == 'Bid']\n",
    "punk_1_offered = punk_1_trans[punk_1_trans['trans'] == 'Offered']\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=punk_1_offered['date'],\n",
    "    y=punk_1_offered['usd'],\n",
    "    xperiod=\"M1\",\n",
    "    xperiodalignment=\"middle\",\n",
    "    hovertemplate=\"Price: $%{y}k<br>Date: %{x}\"\n",
    "    # hover_data={\"date\": \"|%B %d, %Y\"}\n",
    "))\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True,\n",
    "                rangeselector=dict(\n",
    "                    buttons=list([dict(count=1, label=\"1 month\", step=\"month\", stepmode=\"backward\"),\n",
    "                                  dict(count=6, label=\"6 months\", step=\"month\", stepmode=\"backward\"),\n",
    "                                  # dict(count=1, label=\"Year to Date\", step=\"year\", stepmode=\"todate\"),\n",
    "                                  dict(count=1, label=\"1 Year\", step=\"year\", stepmode=\"backward\"),\n",
    "                                  dict(label= 'All', step=\"all\")])))\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "fig.update_layout(xaxis_range=['2017-06-23','2021-11-23'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e1906-6736-4d25-ab90-8a01e38ace2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Get Average and Max Sales, Bids, and Offers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776b73b-1183-497d-8d39-264140865998",
   "metadata": {},
   "source": [
    "Average Price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26b001-dd43-4550-8efb-f64287e27fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sale = df[df['trans'] == 'Sold']\n",
    "df_sale = df_sale[['punk_id', 'usd']]\n",
    "\n",
    "df_avg_sale = df_sale.groupby(\"punk_id\")[['usd']].mean()\n",
    "df_avg_sale.reset_index(inplace = True)\n",
    "df_avg_sale.rename(columns = {'usd':'avg_usd_sale'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb0a7c6-1acf-4b0f-b5a3-dd6b6d25921a",
   "metadata": {},
   "source": [
    "Max Sale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0f8f9-de06-4fd4-a8ac-178da0a5e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_sale = df_sale.groupby(\"punk_id\")[['usd']].max()\n",
    "df_max_sale.reset_index(inplace = True)\n",
    "df_max_sale.rename(columns = {'usd':'max_usd_sale'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40217fdf-317a-406d-bd86-1ee9ef03c24c",
   "metadata": {},
   "source": [
    "Average Bid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcfc3d-f400-449e-a03f-4e4844439125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bid = df[df['trans'] == 'Bid']\n",
    "df_bid = df_bid[['punk_id', 'usd']]\n",
    "\n",
    "df_avg_bid = df_bid.groupby(\"punk_id\")[['usd']].mean()\n",
    "df_avg_bid.reset_index(inplace = True)\n",
    "df_avg_bid.rename(columns = {'usd':'avg_usd_bid'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd219698-8e40-4a45-b793-4a389379f3d5",
   "metadata": {},
   "source": [
    "Max Bid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae23797-1109-4180-b65f-1b3bc19c3b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_bid = df_bid.groupby(\"punk_id\")[['usd']].max()\n",
    "df_max_bid.reset_index(inplace = True)\n",
    "df_max_bid.rename(columns = {'usd':'max_usd_bid'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ac3ec-d9c9-4302-8322-5adc691a9bcd",
   "metadata": {},
   "source": [
    "Average Offer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab50fd1-ef6a-4e36-bd2b-a35432c56f0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_offer = df[df['trans'] == 'Offered']\n",
    "df_offer = df_offer[['punk_id', 'usd']]\n",
    "\n",
    "df_avg_offer = df_offer.groupby(\"punk_id\")[['usd']].mean()\n",
    "df_avg_offer.reset_index(inplace = True)\n",
    "df_avg_offer.rename(columns = {'usd':'avg_usd_offer'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d9f5c-d655-4d95-a3b2-97e6dcae0fed",
   "metadata": {},
   "source": [
    "Max Offer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d9f91-f924-4d09-8394-c038ad378011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_offer = df_offer.groupby(\"punk_id\")[['usd']].max()\n",
    "df_max_offer.reset_index(inplace = True)\n",
    "df_max_offer.rename(columns = {'usd':'max_usd_offer'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c79ec5e-d029-4190-94a2-72dca4b63246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9f919-15a7-4c23-891b-991a76deaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://goodboychan.github.io/python/datacamp/natural_language_processing/2020/07/17/04-TF-IDF-and-similarity-scores.html\n",
    "\n",
    "def get_recommendations(punk_id):\n",
    "    \n",
    "    # check if punk is female or male to ensure the normal types are grouped together,\n",
    "    # as the normal types sell for far lower than the special types\n",
    "    if (new_indiv['type'].iloc[punk_id] == 'female') | (new_indiv['type'].iloc[punk_id] == 'male'):\n",
    "        print(\"normal\")\n",
    "        indices = pd.Series(normal_types_indiv.index, index=normal_types_indiv['punk_id'])\n",
    "    else:\n",
    "        indices = pd.Series(special_types_indiv.index, index=special_types_indiv['punk_id'])\n",
    "        print(\"special\")\n",
    "        \n",
    "    # get accessories list column\n",
    "    acc_list = special_types_indiv['clean_accessories']\n",
    "        \n",
    "    # instantiate tfidf\n",
    "    tfidf = TfidfVectorizer()\n",
    "\n",
    "    # construct the TF-IDF matrix\n",
    "    tfidf_matrix = tfidf.fit_transform(acc_list)\n",
    "\n",
    "    # generate the cosine similarity matrix\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # get the index of the punk that matches the punk_id\n",
    "    idx = indices[punk_id]\n",
    "    \n",
    "    # get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    \n",
    "    # sort the punks based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "   \n",
    "    # get the scores for 10 most similar punks\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    \n",
    "    # get the punk indices\n",
    "    punk_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # return the top 10 most similar punks\n",
    "    # return new_rarity['punk_id'].iloc[punk_indices]\n",
    "    \n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0441de-f2ae-4949-aa3a-9f5523715fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://goodboychan.github.io/python/datacamp/natural_language_processing/2020/07/17/04-TF-IDF-and-similarity-scores.html\n",
    "\n",
    "def get_recommendations(punk_id):\n",
    "    \n",
    "    # get indices\n",
    "    indices = pd.Series(new_indiv.index, index=new_indiv['punk_id'])\n",
    "        \n",
    "    # get accessories list column\n",
    "    acc_list = new_indiv['clean_accessories']\n",
    "        \n",
    "    # instantiate tfidf\n",
    "    tfidf = TfidfVectorizer()\n",
    "\n",
    "    # construct the TF-IDF matrix\n",
    "    tfidf_matrix = tfidf.fit_transform(acc_list)\n",
    "\n",
    "    # generate the cosine similarity matrix\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # get the index of the punk that matches the punk_id\n",
    "    idx = indices[punk_id]\n",
    "    \n",
    "    # get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    \n",
    "    # sort the punks based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # get the punk indices\n",
    "    punk_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # empty list to add to\n",
    "    type_sim_scores = []\n",
    "   \n",
    "    # if normal type only want to return normal types\n",
    "    if (new_indiv['type'].iloc[punk_id] == 'female') | (new_indiv['type'].iloc[punk_id] == 'male'):\n",
    "        # get the scores for 10 most similar punks\n",
    "        for count, value_id in enumerate(punk_indices):\n",
    "            if (new_indiv['type'].iloc[value_id] == 'female') | (new_indiv['type'].iloc[value_id] == 'male'):\n",
    "                type_sim_scores.append(sim_scores[value_id])\n",
    "    # if special type only want to return special types\n",
    "    else:\n",
    "        for count, value_id in enumerate(punk_indices):\n",
    "            if (new_indiv['type'].iloc[value_id] == 'ape') | (new_indiv['type'].iloc[value_id] == 'alien') | (new_indiv['type'].iloc[value_id] == 'zombie'):\n",
    "                type_sim_scores.append(sim_scores[value_id])\n",
    "\n",
    "    type_sim_scores = type_sim_scores[1:11]\n",
    "    # return the top 10 most similar punks\n",
    "    # return new_rarity['punk_id'].iloc[punk_indices]\n",
    "    \n",
    "    return type_sim_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661cfe0-4985-43ed-8037-fbf6a1bde27b",
   "metadata": {},
   "source": [
    "Highest Sale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a854a3f-ee61-42ff-8443-4f9addf10269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_sale = df_sale.groupby(\"punk_id\")[['usd']].max()\n",
    "df_max_sale.reset_index(inplace = True)\n",
    "df_max_sale.rename(columns = {'usd':'max_usd_sale'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e030db-cfa4-488c-acbc-c30eecf16d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88d815d-484b-4762-afc8-3944adb0a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'ridge__alpha' : [.01, .1, 1, 10, 100],\n",
    "# }\n",
    "\n",
    "# ct1 = make_column_transformer(\n",
    "#     (OneHotEncoder(handle_unknown = 'ignore', sparse=False), make_column_selector(dtype_include = object)), \n",
    "#     remainder = 'passthrough'\n",
    "# )\n",
    "\n",
    "# ct1\n",
    "\n",
    "# pipe = make_pipeline(ct1,StandardScaler(),Ridge())\n",
    "\n",
    "# gs = GridSearchCV(pipe, params, n_jobs=-1)\n",
    "# gs.fit(X_train, y_train)\n",
    "\n",
    "# ValueError: X has 19 features, but StandardScaler is expecting 15 features as input.\n",
    "\n",
    "# gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc6ec2-a69c-4185-99c8-175f60d0d4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
